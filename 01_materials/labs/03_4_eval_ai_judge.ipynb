{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe851e5",
   "metadata": {},
   "source": [
    "# AI as Judge\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM as a judge to evaluate LLM outputs. The evaluation can be based on any criteria. G-Eval is implemented by a library called [DeepEval](https://deepeval.com/) which includes a broader set of tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0650f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd5ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "document_folder = \"../../05_src/documents/\"\n",
    "blue_cross_file = \"chesterton.txt\"\n",
    "file_path = os.path.join(document_folder, blue_cross_file)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    blue_cross_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843da553",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that summarizes works of fiction with a quirky and bubbly approach.\"\n",
    "PROMPT = \"\"\"\n",
    "    Summarize the following story in at most four paragraphs. Please include all key characters and plot points.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    In addition to the summary, add an introduction paragraph where you greet the reader and a conclusion where you share an opinion about the story.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f951529",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=1.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269743e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Hello, fellow book lovers!** ‚ú® Are you ready to immerse yourself in a delightful blend of mystery and quirkiness? Welcome to the charmingly puzzling tales of *The Innocence of Father Brown* by G. K. Chesterton, where Father Brown, our endearing little detective priest, uses his sharp wits and compassion to unravel the murkiest of crimes! Now, let's unpack this marvelous collection, shall we?\\n\\nIn the opener, ‚ÄúThe Blue Cross,‚Äù we meet Aristide Valentin, a clever French detective, who embarks on a quest to catch the infamous criminal Flambeau, only to hilariously discover that the scruffy priest he has overlooked‚ÄîFather Brown‚Äîmight just be the key to his capture. As their paths intertwine, the dark-and-dim Church juxtaposing the shenanigans of childhood conversations lead to deeper reflections on morality and humanity. Packing a punch with lively characters and shrewd observations, these early exchanges set the stage for both crime and curiosity!\\n\\nThrough eight more whimsy-laden tales, Chesterton guides us through delightful plots such as ‚ÄúThe Incorrecthelves,‚Äù which showcases Father Brown exposing a laughably surreal businessman married to the notion of love. But don‚Äôt swipe the pages too fast! Each chapter explores a new layer of humanity, from love and justice to the unabashed realities of human failures. ‚ÄúThe Hammer of God‚Äù introduces us to the boisterous Colonel Bohun, capturing the peculiar intersections of folly and salvation when a tumultuous murder unravels among family ties.\\n\\nIn conclusion, *The Innocence of Father Brown* sparkles with wit and wisdom, reminding us there‚Äôs always more than meets the eye! üìñ‚ú® Our charming priest shows that beneath every snazzy facade lies vulnerability, drowning under overshadowing questions of good versus evil. Chesterton‚Äôs endearing blend of humor merged with profound theories on sin creates an enchanting dialogue that I absolutely adored! üöÄ So grab a cozy blanket and take your time with this sweet trove of stories‚Äîyou won‚Äôt regret a single nibble from this literary magic pot! \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae503b",
   "metadata": {},
   "source": [
    "# Answer Relevancy\n",
    "\n",
    "The answer relevancy metric evaluates how relevant the actual output of the LLM app is compared to the provided input. This metric is self-explaining in the sense that the output includes a reason for the metric score.\n",
    "\n",
    "The metric is calculated as:\n",
    "\n",
    "$$\n",
    "AnswerRelevancy=\\frac{NumberRelevantStatements}{TotalStatements}\n",
    "$$\n",
    "\n",
    "Reference: [Answer Relevancy](https://deepeval.com/docs/metrics-answer-relevancy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7de7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.measure(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric.score,metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa92303",
   "metadata": {},
   "source": [
    "# Other Metrics\n",
    "\n",
    "Other useful metric functions include:\n",
    "\n",
    "+ [Faithfulness](https://deepeval.com/docs/metrics-faithfulness): evaluates whether the `actual_output` factually aligns with the contents of  `retrieval_context`. \n",
    "+ [Contextual Precision](https://deepeval.com/docs/metrics-contextual-precision): evaluates whether nodes in your `retrieval_context` that are relevant to the given input are ranked higher than irrelevant ones. \n",
    "+ [Contextual Recall](https://deepeval.com/docs/metrics-contextual-recall): evaluates the extent of which the retrieval_context aligns with the expected_output. \n",
    "+ [Contextual Relevancy](https://deepeval.com/docs/metrics-contextual-relevancy): evaluates the overall relevance of the information presented in your retrieval_context for a given input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7b6ba",
   "metadata": {},
   "source": [
    "# G-Eval\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM-as-a-judge with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria. The G-Eval metric is the most versatile type of metric deepeval offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that specializes in works of fiction.\"\n",
    "PROMPT = \"\"\"\n",
    "    Based on the story below, answer the question provided.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    <question>\n",
    "    Who is the main antagonist in the story and what motivates their actions?\n",
    "    </question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df04e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0eb8ad",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "The most straightforward way to establish a metric is by using a single criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a310a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the context.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0312796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9542714",
   "metadata": {},
   "source": [
    "## Evaluation Steps \n",
    "\n",
    "G-Eval is flexible in many ways: notice that we can establish an evaluation criteria or a set of evaluation steps, that can help in guiding the model to follow specific steps to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8647c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'input'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are not OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e44fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
